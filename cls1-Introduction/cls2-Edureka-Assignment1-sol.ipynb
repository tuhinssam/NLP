{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Write a program to enter a string from user and perform following tasks\n",
    "•\tWrite a python function named “Tokenize” which returns the tokenized string\n",
    "•\tPrint tokens along with the frequency of each token using the above function\n",
    "•\tPrint the 5 least occurring tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to Natural Language Processing course. It has many Natural language processing techniques and also includes text mining . It will show various examples on text processing.\n"
     ]
    }
   ],
   "source": [
    "#Inputting string from user \n",
    "inputted_string=raw_input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write a python function named “Tokenize” which returns the tokenized string\n",
    "import nltk\n",
    "def Tokenize(string):\n",
    "    tokens=nltk.tokenize.word_tokenize(string)\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'and': 1, 'mining': 1, 'Welcome': 1, 'Natural': 2, 'Language': 1, 'show': 1, 'text': 2, 'processing': 2, 'includes': 1, 'course': 1, 'examples': 1, 'techniques': 1, 'on': 1, 'language': 1, 'also': 1, 'many': 1, 'Processing': 1, 'It': 2, '.': 3, 'will': 1, 'to': 1, 'various': 1, 'has': 1}\n"
     ]
    }
   ],
   "source": [
    "#Print tokens along with the frequency of each token using the above function\n",
    "inputted_string_tokens=Tokenize(inputted_string).split(\" \")\n",
    "from collections import Counter\n",
    "count_of_all_tokens=dict(Counter(inputted_string_tokens))\n",
    "print count_of_all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".: 3\n",
      "text: 2\n",
      "processing: 2\n",
      "Natural: 2\n",
      "It: 2\n"
     ]
    }
   ],
   "source": [
    "#Print the 5 least occurring tokens \n",
    "for key, value in sorted(count_of_all_tokens.iteritems(), key=lambda (k,v): (v,k),reverse=True)[:5]:\n",
    "    print \"%s: %s\" % (key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2. Write a program to enter a string from user and perform following tasks.\n",
    "•\tWrite a python function named “RemoveStopWords” which returns the string after removing stop words\n",
    "•\tCount frequency of each stop word present in a string using the above function\n",
    "•\tPlot a bar graph depicting stop words and their frequencies \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to Natural Language Processing course. It has many Natural language processing techniques and also includes text mining . It will show various examples on text processing.\n"
     ]
    }
   ],
   "source": [
    "#Inputting string from user \n",
    "inputted_string=raw_input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write a python function named “RemoveStopWords” which returns the string after removing stop words\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "def RemoveStopWords(string):\n",
    "    #Removing Punctuations\n",
    "    for each in punctuation:\n",
    "        string=string.replace(each,\"\")\n",
    "    \n",
    "    #Removing Stopwords\n",
    "    english_stopwords=stopwords.words('english')\n",
    "    stopwords_removed_tokens=[]\n",
    "    words=string.split(\" \")\n",
    "    \n",
    "    for each in words:\n",
    "        if each not in english_stopwords:\n",
    "            stopwords_removed_tokens.append(each)\n",
    "    return \" \".join(stopwords_removed_tokens)   \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('and', 1), ('on', 1), ('It', 2), ('will', 1), ('to', 1), ('has', 1)]\n"
     ]
    }
   ],
   "source": [
    "#Count frequency of each stop word present in a string using the above function\n",
    "english_stopwords=stopwords.words('english')\n",
    "stopwords_in_string=[]\n",
    "tokens=nltk.tokenize.word_tokenize(inputted_string) \n",
    "for each in tokens:\n",
    "    if each.lower() in english_stopwords:\n",
    "        stopwords_in_string.append(each)\n",
    "stopwords_in_string=list(set(stopwords_in_string))\n",
    "\n",
    "count_of_stopwords=map(lambda x:inputted_string.count(x),stopwords_in_string)\n",
    "print zip(stopwords_in_string,count_of_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXEAAAEACAYAAABF+UbAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSY0ZcUlqzIhLUmP/C5RDSYjXoSrKAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11f19a20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot a bar graph depicting stop words and their frequencies\n",
    "import matplotlib.pyplot as plt\n",
    "plt.bar(range(len(stopwords_in_string)),count_of_stopwords)\n",
    "plt.xticks(range(len(stopwords_in_string)),stopwords_in_string)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Write a program to enter a string from user and perform following tasks \n",
    "•\tWrite a python function named “Lemmatize” which returns a string after lemmatizing the string.\n",
    "•\tWrite a python function named “Stemmed” which returns a string after stemming the string. \n",
    "(Use any stemmer of your preference)\n",
    "•\tPrint all the words along with their lemmatized and stemmed form using the above functions\n",
    "•\tSave these results in a csv file having 3 columns:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Inputting string from user \n",
    "inputted_string=raw_input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write a python function named “Lemmatize” which returns a string after lemmatizing the string.\n",
    "from nltk.stem import  wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "def Lemmatize(string):\n",
    "    word_lem=WordNetLemmatizer()\n",
    "    words=string.split() \n",
    "    lemmatizeWords=[]\n",
    "    for each in words:\n",
    "        lemmatizeWords.append(word_lem.lemmatize(each))\n",
    "    return \" \".join(lemmatizeWords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write a python function named “Stemmed” which returns a string after stemming the string.\n",
    "from nltk.stem import PorterStemmer\n",
    "def Stemmed(string):\n",
    "    pst=PorterStemmer()\n",
    "    words=string.split() \n",
    "    stemmedWords=[]\n",
    "    for each in words:\n",
    "        stemmedWords.append(pst.stem(each))\n",
    "    return \" \".join(stemmedWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Form:\n",
      "Welcome to Natural Language Processing course. It ha many Natural language processing technique and also includes text mining . It will show various example on text processing.\n",
      "\n",
      "Stemmed Form:\n",
      "welcom to natur languag process course. It ha mani natur languag process techniqu and also includ text mine . It will show variou exampl on text processing.\n"
     ]
    }
   ],
   "source": [
    "#Print all the words along with their lemmatized and stemmed form using the above functions\n",
    "\n",
    "#Lemmatization Results\n",
    "print (\"Lemmatized Form:\")\n",
    "print Lemmatize(inputted_string)\n",
    "\n",
    "print \n",
    "#Stemming Results\n",
    "\n",
    "print (\"Stemmed Form:\")\n",
    "print Stemmed(inputted_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save these results in a csv file having 3 columns:\n",
    "lemmatize_words=Lemmatize(inputted_string).split(\" \")\n",
    "stem_words=Stemmed(inputted_string).split(\" \")\n",
    "original_words=inputted_string.split(\" \")\n",
    "result=[]\n",
    "for each in zip(original_words,lemmatize_words,stem_words):\n",
    "    result.append(each)\n",
    "    \n",
    "import pandas as pd\n",
    "df=pd.DataFrame(result,columns=['Original Form','Lemmatized Form','Stemmed Form'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('WordsWithLemmatizedAndStemmedForm.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Create a python file named “PreProcess” and perform the following tasks.\n",
    "•\tCopy the function “Tokenize” in this file from question 1\n",
    "•\tCopy the function “RemoveStopWords” in this file from question 2\n",
    "•\tCopy the function “Lemmatize” in this file from question 3\n",
    "Create a function named “Refine” which accepts a string and call the above 3 functions in the same order i.e. first Tokenize then RemoveStopWords then Lemmatize.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk.stem import  wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#Tokenize Function\n",
    "def Tokenize(string):\n",
    "    tokens=nltk.tokenize.word_tokenize(string)\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "#RemoveStopWordsFunction\n",
    "def RemoveStopWords(string):\n",
    "    #Removing Punctuations\n",
    "    for each in punctuation:\n",
    "        string=string.replace(each,\"\")\n",
    "    \n",
    "    #Removing Stopwords\n",
    "    english_stopwords=stopwords.words('english')\n",
    "    stopwords_removed_tokens=[]\n",
    "    words=string.split(\" \")\n",
    "    \n",
    "    for each in words:\n",
    "        if each not in english_stopwords:\n",
    "            stopwords_removed_tokens.append(each)\n",
    "    return \" \".join(stopwords_removed_tokens) \n",
    "\n",
    "\n",
    "#LemmatizeFunction\n",
    "def Lemmatize(string):\n",
    "    word_lem=WordNetLemmatizer()\n",
    "    words=string.split() \n",
    "    lemmatizeWords=[]\n",
    "    for each in words:\n",
    "        lemmatizeWords.append(word_lem.lemmatize(each))\n",
    "    return \" \".join(lemmatizeWords)\n",
    "\n",
    "def Refine(string):\n",
    "    return Lemmatize(RemoveStopWords(Tokenize(string)))\n",
    "\n",
    "#Save the above functions in a file called PreProcess.py "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
