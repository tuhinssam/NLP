{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Write a program to input three sentences from user and creates the corpus\n",
    "\n",
    "Create a function named “MakeCorpus” which will take list of string as an input and will return a list having union of all words. Save this function in a python file named “Corpus”. This can be used for future applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: India won the match\n",
      "Sentence 2: England won the cricket match\n",
      "Sentence 3: Australia won the final match\n"
     ]
    }
   ],
   "source": [
    "#Inputting string from user \n",
    "sent1=raw_input(\"Sentence 1: \")\n",
    "sent2=raw_input(\"Sentence 2: \")\n",
    "sent3=raw_input(\"Sentence 3: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "def MakeCorpus(list_of_strings):\n",
    "    allWords=[]\n",
    "    for each in list_of_strings:\n",
    "        words=nltk.word_tokenize(each)\n",
    "        for every in words:\n",
    "            allWords.append(every)\n",
    "    corpus=list(set(allWords))\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cricket', 'Australia', 'England', 'India', 'won', 'the', 'final', 'match']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_inputs=[sent1,sent2,sent3]\n",
    "MakeCorpus(list_of_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Write a program to input three sentences from user and convert them into vectors.    Use presence and absence of words to build the vectors.\n",
    "\n",
    "Create a function named “PresenceAbsenceVectorization” which will take list of string as an input and will return a list of vectors. Save this function in a python file named “Vectorization”. This can be used for future applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def PresenceAbsenceVectorization(list_of_string):\n",
    "    corpus=MakeCorpus(list_of_string)\n",
    "    list_of_vectors=[]\n",
    "    for each in list_of_string:\n",
    "        words=nltk.word_tokenize(each)\n",
    "        vector=[]\n",
    "        for every in corpus:\n",
    "            if every in words:\n",
    "                vector.append(1)\n",
    "            else:\n",
    "                vector.append(0)\n",
    "        list_of_vectors.append(vector)\n",
    "    return list_of_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0, 1, 1, 1, 0, 1], [1, 0, 1, 0, 1, 1, 0, 1], [0, 1, 0, 0, 1, 1, 1, 1]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PresenceAbsenceVectorization(list_of_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Write a program to enter 3 strings from a user and vectorise them on basis of their counts.  \n",
    "\n",
    "\n",
    "Create a function named “CountVectorization” which will take list of string as an input and will return a list of vectors. Save this function in a python file named “Vectorization”. This can be used for future applications\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def CountVectorization(list_of_string):\n",
    "    corpus=MakeCorpus(list_of_string)\n",
    "    list_of_vectors=[]\n",
    "    for each in list_of_string:\n",
    "        words=nltk.word_tokenize(each)\n",
    "        vector=[]\n",
    "        for every in corpus:\n",
    "            if every in words:\n",
    "                vector.append(words.count(every))\n",
    "            else:\n",
    "                vector.append(0)\n",
    "        list_of_vectors.append(vector)\n",
    "    return list_of_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 1, 1, 1, 0, 1, 1, 2], [0, 1, 2, 1, 1, 1, 1, 2], [1, 2, 0, 1, 1, 1, 1, 2]]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CountVectorization([\"A lives with B. A plays with C\",\"B lives with C . B plays with D\",\"C lives with D . C plays with A\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Write a program to input 3 strings but vectorise them using TF-IDF (Term Frequency and Inverse Document Frequency) and print the strings along with the vectors.\n",
    "\n",
    "Create a function named “TFIDFVectorization” which will take list of string as an input and will return a list of vectors. Save this function in a python file named “Vectorization”. This can be used for future applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def TFIDFVectorization(list_of_string):\n",
    "    vectorizer=TfidfVectorizer()\n",
    "    tf_idfMatrix=vectorizer.fit_transform(list_of_string).toarray()\n",
    "    return list(tf_idfMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.69903033,\n",
       "         0.41285857,  0.41285857,  0.41285857]),\n",
       " array([ 0.        ,  0.57292883,  0.57292883,  0.        ,  0.        ,\n",
       "         0.338381  ,  0.338381  ,  0.338381  ]),\n",
       " array([ 0.57292883,  0.        ,  0.        ,  0.57292883,  0.        ,\n",
       "         0.338381  ,  0.338381  ,  0.338381  ])]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TFIDFVectorization(list_of_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "def MakeCorpus(list_of_strings):\n",
    "    allWords=[]\n",
    "    for each in list_of_strings:\n",
    "        words=nltk.word_tokenize(each)\n",
    "        for every in words:\n",
    "            allWords.append(every)\n",
    "    corpus=list(set(allWords))\n",
    "    return corpus\n",
    "\n",
    "def PresenceAbsenceVectorization(list_of_string):\n",
    "    corpus=MakeCorpus(list_of_string)\n",
    "    list_of_vectors=[]\n",
    "    for each in list_of_string:\n",
    "        words=nltk.word_tokenize(each)\n",
    "        vector=[]\n",
    "        for every in corpus:\n",
    "            if every in words:\n",
    "                vector.append(1)\n",
    "            else:\n",
    "                vector.append(0)\n",
    "        list_of_vectors.append(vector)\n",
    "    return list_of_vectors\n",
    "\n",
    "def CountVectorization(list_of_string):\n",
    "    corpus=MakeCorpus(list_of_string)\n",
    "    list_of_vectors=[]\n",
    "    for each in list_of_string:\n",
    "        words=nltk.word_tokenize(each)\n",
    "        vector=[]\n",
    "        for every in corpus:\n",
    "            if every in words:\n",
    "                vector.append(words.count(every))\n",
    "            else:\n",
    "                vector.append(0)\n",
    "        list_of_vectors.append(vector)\n",
    "    return list_of_vectors\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def TFIDFVectorization(list_of_string):\n",
    "    vectorizer=TfidfVectorizer()\n",
    "    tf_idfMatrix=vectorizer.fit_transform(list_of_string).toarray()\n",
    "    return list(tf_idfMatrix)\n",
    "\n",
    "#Save the above functions in Vectorization.py \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
