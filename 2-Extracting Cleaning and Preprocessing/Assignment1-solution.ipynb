{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Write a program to enter a string from user and perform following tasks\n",
    "•Write a python functionnamed “Tokenize”which returnsthe tokenized string\n",
    "•Print tokens along with the frequency of each tokenusing the above function\n",
    "•Print the 5 least occurring tokens \n",
    "\n",
    "2.Write a program to enter a string from user and perform following tasks.\n",
    "•Write a python functionnamed “RemoveStopWords”which returns the string after removing stop words\n",
    "•Count frequency ofeach stop word present in a string using the above function\n",
    "•Plot a bar graph depicting stop wordsand their frequencies \n",
    "\n",
    "3.Write a program to enter a string from user and perform following tasks\n",
    "•Write a python functionnamed “Lemmatize”which returns a string after lemmatizing the string.\n",
    "•Write a python functionnamed “Stemmed” which returns a string after stemming the string. (Use any stemmer of your preference)\n",
    "•Print all the words along with their lemmatized and stemmed form using theabove functions\n",
    "•Save these results in a csv file having 3 columns:Original WordLemmatized FormStemmed Form\n",
    "    \n",
    "4.Create a python file named “PreProcess” and perform the following tasks.\n",
    "•Copy the function “Tokenize” in this file from question 1\n",
    "•Copy thefunction “RemoveStopWords” in this file from question 2\n",
    "•Copy the function “Lemmatize” in this file from question 3Create a function named “Refine” which accepts a string and call the above 3 functions in the same order i.e. first Tokenize then RemoveStopWords then Lemmatize.Remember:>Inputted string will be input to Tokenize Function> Tokenized String will be input to RemoveStopWords function> StopWordsRemoved string will be input to Lemmatize functionSave this python file as PreProcess and you can use this for upcoming assignments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a string to analyse:Write a program to enter a string from user and perform following tasks •Write a python functionnamed “Tokenize”which returnsthe tokenized string •Print tokens along with the frequency of each tokenusing the above function •Print the 5 least occurring tokens \n"
     ]
    }
   ],
   "source": [
    "userinput = input(\"Enter a string to analyse:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tokenize(userinput):\n",
    "    word_tokens = word_tokenize(userinput)\n",
    "    print(\"Number of Tokens: \",len(word_tokens))\n",
    "    \n",
    "    print(\"Frequency Distributions:\")\n",
    "    fdist = FreqDist()\n",
    "    for token in word_tokens:\n",
    "        fdist[token.lower()] +=1\n",
    "        \n",
    "    #print freq distributions   \n",
    "    for freq in fdist:\n",
    "        print(freq, fdist[freq])\n",
    "    print()\n",
    "    print(\"get 5 least frequent words:\")   \n",
    "    #get 5 least frequent words\n",
    "    print(fdist.most_common()[-5:])\n",
    "    \n",
    "    return word_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Tokens:  42\n",
      "Frequency Distributions:\n",
      "write 1\n",
      "a 3\n",
      "program 1\n",
      "to 1\n",
      "enter 1\n",
      "string 2\n",
      "from 1\n",
      "user 1\n",
      "and 1\n",
      "perform 1\n",
      "following 1\n",
      "tasks 1\n",
      "•write 1\n",
      "python 1\n",
      "functionnamed 1\n",
      "“ 1\n",
      "tokenize 1\n",
      "” 1\n",
      "which 1\n",
      "returnsthe 1\n",
      "tokenized 1\n",
      "•print 2\n",
      "tokens 2\n",
      "along 1\n",
      "with 1\n",
      "the 3\n",
      "frequency 1\n",
      "of 1\n",
      "each 1\n",
      "tokenusing 1\n",
      "above 1\n",
      "function 1\n",
      "5 1\n",
      "least 1\n",
      "occurring 1\n",
      "\n",
      "get 5 least frequent words:\n",
      "[('above', 1), ('function', 1), ('5', 1), ('least', 1), ('occurring', 1)]\n"
     ]
    }
   ],
   "source": [
    "tokens = Tokenize(userinput)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Write a program to enter a string from user and perform following tasks.\n",
    "•Write a python functionnamed “RemoveStopWords”which returns the string after removing stop words\n",
    "•Count frequency ofeach stop word present in a string using the above function\n",
    "•Plot a bar graph depicting stop wordsand their frequencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a string:Overall 7 years of Information Technology and Services experience with extensive experience in Robotic Process Automation using UiPath RPA Tools.  Experience in developing and managing RPA solutions using Robotic Enterprise Framework (REFramework), a framework for high quality automation delivery.  Experience in automating Field Service for Utility and Telecommunication Customers using Automation Agents.  Worked with Infosys Ltd. as a Senior Associate Consultant from December, 2015 to Now  Worked in Ericsson India Global Services Pvt Ltd as a Solution Integrator from November 2012 to December, 2015 (3 Years 2 Months). \n"
     ]
    }
   ],
   "source": [
    "inpstr2 = input(\"Enter a string:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RemoveStopWords(inpstr2):\n",
    "    punctuations = re.compile(r'[-.?!,:;()|0-9]')\n",
    "    post_punc = []\n",
    "    stopwords =[]\n",
    "    stopword_counts = []\n",
    "    fdist = FreqDist()\n",
    "    for word_token in word_tokenize(inpstr2):\n",
    "        word = punctuations.sub(\"\", word_token)\n",
    "        if word_token != word:\n",
    "            fdist[word_token]+=1\n",
    "        if len(word)>0:\n",
    "            post_punc.append(word)\n",
    "    for stpword in fdist:\n",
    "        print(stpword, fdist[stpword])\n",
    "        stopwords.append(stpword)\n",
    "        stopword_counts.append(fdist[stpword])\n",
    "    plt.bar(stopwords,stopword_counts)\n",
    "    return \" \".join(post_punc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 1\n",
      ". 4\n",
      "( 2\n",
      ") 2\n",
      ", 3\n",
      "Ltd. 1\n",
      "2015 2\n",
      "2012 1\n",
      "3 1\n",
      "2 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Overall years of Information Technology and Services experience with extensive experience in Robotic Process Automation using UiPath RPA Tools \\uf0d8 Experience in developing and managing RPA solutions using Robotic Enterprise Framework REFramework a framework for high quality automation delivery \\uf0d8 Experience in automating Field Service for Utility and Telecommunication Customers using Automation Agents \\uf0d8 Worked with Infosys Ltd as a Senior Associate Consultant from December to Now \\uf0d8 Worked in Ericsson India Global Services Pvt Ltd as a Solution Integrator from November to December Years Months'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "RemoveStopWords(inpstr2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.Write a program to enter a string from user and perform following tasks\n",
    "•Write a python functionnamed “Lemmatize”which returns a string after lemmatizing the string.\n",
    "•Write a python functionnamed “Stemmed” which returns a string after stemming the string. (Use any stemmer of your preference)\n",
    "•Print all the words along with their lemmatized and stemmed form using theabove functions\n",
    "•Save these results in a csv file having 3 columns:Original WordLemmatized FormStemmed Form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter input string: Write a program to enter\n"
     ]
    }
   ],
   "source": [
    "inpstr3 = input(\"Enter input string: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "        \"\"\"\n",
    "        return WORDNET POS compliance to WORDENT lemmatization (a,n,r,v) \n",
    "        \"\"\"\n",
    "        #if treebank_tag.startswith('J'):\n",
    "            #return wordnet.ADJ\n",
    "        #elif treebank_tag.startswith('V'):\n",
    "            #return wordnet.VERB\n",
    "        if treebank_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        #elif treebank_tag.startswith('R'):\n",
    "            #return wordnet.ADV\n",
    "        else:\n",
    "            # As default pos in lemmatization is Noun\n",
    "            return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Lemmatize(inpstr3):\n",
    "    word_lem = WordNetLemmatizer()\n",
    "    for word_token in word_tokenize(inpstr3):\n",
    "        postagged = pos_tag([word_token])\n",
    "        print(postagged[0][1])\n",
    "        posval = get_wordnet_pos(postagged[0][1])\n",
    "        print(word_token + \": \"+ word_lem.lemmatize(word_token, posval)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VB\n",
      "Write: Write\n",
      "DT\n",
      "a: a\n",
      "NN\n",
      "program: program\n",
      "TO\n",
      "to: to\n",
      "NN\n",
      "enter: enter\n"
     ]
    }
   ],
   "source": [
    "Lemmatize(inpstr3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "def Stemmed(inpstr3):\n",
    "    ps = PorterStemmer()\n",
    "    for word in word_tokenize(inpstr3):\n",
    "        print(word+\": \"+ps.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write: write\n",
      "a: a\n",
      "program: program\n",
      "to: to\n",
      "enter: enter\n"
     ]
    }
   ],
   "source": [
    "Stemmed(inpstr3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExportToCSV(inpstr3):\n",
    "    word_lem = WordNetLemmatizer()\n",
    "    ps = PorterStemmer()\n",
    "    word_lst = []\n",
    "    stemed_lst =[]\n",
    "    lemm_lst = []\n",
    "    for word_token in word_tokenize(inpstr3):\n",
    "        word_lst.append(word_token)\n",
    "        stemed_lst.append(ps.stem(word_token))\n",
    "        postagged = pos_tag([word_token])\n",
    "        #print(postagged[0][1])\n",
    "        posval = get_wordnet_pos(postagged[0][1])\n",
    "        #print(word_token + \": \"+ word_lem.lemmatize(word_token, posval))\n",
    "        lemm_lst.append(word_lem.lemmatize(word_token, posval))\n",
    "    data = {'Word':word_lst, \"Stemmed\": stemed_lst, \"Lemmitized\": lemm_lst}\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(\"file1.csv\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "ExportToCSV(inpstr3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.Create a python file named “PreProcess” and perform the following tasks.\n",
    ">Copy the function “Tokenize” in this file from question 1\n",
    ">Copy thefunction “RemoveStopWords” in this file from question 2\n",
    ">Copy the function “Lemmatize” in this file from question 3Create a function named “Refine” which accepts a string and call the above 3 functions in the same order i.e. first Tokenize then RemoveStopWords then Lemmatize.Remember:>Inputted string will be input to Tokenize Function> Tokenized String will be input to RemoveStopWords function> StopWordsRemoved string will be input to Lemmatize functionSave this python file as PreProcess and you can use this for upcoming assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tokenize(userinput):\n",
    "    word_tokens = word_tokenize(userinput)\n",
    "    print(\"Number of Tokens: \",len(word_tokens))\n",
    "    \n",
    "    print(\"Frequency Distributions:\")\n",
    "    fdist = FreqDist()\n",
    "    for token in word_tokens:\n",
    "        fdist[token.lower()] +=1\n",
    "        \n",
    "    #print freq distributions   \n",
    "    for freq in fdist:\n",
    "        print(freq, fdist[freq])\n",
    "    print()\n",
    "    print(\"get 5 least frequent words:\")   \n",
    "    #get 5 least frequent words\n",
    "    print(fdist.most_common()[-5:])\n",
    "    \n",
    "    return word_tokens\n",
    "\n",
    "def RemoveStopWords(word_tokens):\n",
    "    punctuations = re.compile(r'[-.?!,:;()|0-9]')\n",
    "    post_punc = []\n",
    "    stopwords =[]\n",
    "    stopword_counts = []\n",
    "    fdist = FreqDist()\n",
    "    for word_token in word_tokens:\n",
    "        word = punctuations.sub(\"\", word_token)\n",
    "        if word_token != word:\n",
    "            fdist[word_token]+=1\n",
    "        if len(word)>0:\n",
    "            post_punc.append(word)\n",
    "    for stpword in fdist:\n",
    "        print(stpword, fdist[stpword])\n",
    "        stopwords.append(stpword)\n",
    "        stopword_counts.append(fdist[stpword])\n",
    "    plt.bar(stopwords,stopword_counts)\n",
    "    return \" \".join(post_punc) \n",
    "\n",
    "def Lemmatize(inpstr3):\n",
    "    word_lem = WordNetLemmatizer()\n",
    "    for word_token in word_tokenize(inpstr3):\n",
    "        postagged = pos_tag([word_token])\n",
    "        print(postagged[0][1])\n",
    "        posval = get_wordnet_pos(postagged[0][1])\n",
    "        print(word_token + \": \"+ word_lem.lemmatize(word_token, posval)) \n",
    "\n",
    "def Refine():\n",
    "    userinput = input(\"Enter input string: \")\n",
    "    word_tokens = Tokenize(userinput)\n",
    "    strwostopwords = RemoveStopWords(word_tokens)\n",
    "    Lemmatize(strwostopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter input string: Copy the function “Lemmatize” in this file from question 3Create a function named “Refine” which accepts a string and call the above 3 functions in the same order i.e. first Tokenize then RemoveStopWords then Lemmatize.Remember:>Inputted string will be input to Tokenize Function> Tokenized String will be input to RemoveStopWords function> StopWordsRemoved string will be input to Lemmatize functionSave this python file as PreProcess and you can use this for upcoming assignments.\n",
      "Number of Tokens:  82\n",
      "Frequency Distributions:\n",
      "copy 1\n",
      "the 3\n",
      "function 4\n",
      "“ 2\n",
      "lemmatize 2\n",
      "” 2\n",
      "in 2\n",
      "this 3\n",
      "file 2\n",
      "from 1\n",
      "question 1\n",
      "3create 1\n",
      "a 2\n",
      "named 1\n",
      "refine 1\n",
      "which 1\n",
      "accepts 1\n",
      "string 4\n",
      "and 2\n",
      "call 1\n",
      "above 1\n",
      "3 1\n",
      "functions 1\n",
      "same 1\n",
      "order 1\n",
      "i.e 1\n",
      ". 2\n",
      "first 1\n",
      "tokenize 2\n",
      "then 2\n",
      "removestopwords 2\n",
      "lemmatize.remember 1\n",
      ": 1\n",
      "> 3\n",
      "inputted 1\n",
      "will 3\n",
      "be 3\n",
      "input 3\n",
      "to 3\n",
      "tokenized 1\n",
      "stopwordsremoved 1\n",
      "functionsave 1\n",
      "python 1\n",
      "as 1\n",
      "preprocess 1\n",
      "you 1\n",
      "can 1\n",
      "use 1\n",
      "for 1\n",
      "upcoming 1\n",
      "assignments 1\n",
      "\n",
      "get 5 least frequent words:\n",
      "[('can', 1), ('use', 1), ('for', 1), ('upcoming', 1), ('assignments', 1)]\n",
      "3Create 1\n",
      "3 1\n",
      "i.e 1\n",
      ". 2\n",
      "Lemmatize.Remember 1\n",
      ": 1\n",
      "NN\n",
      "Copy: Copy\n",
      "DT\n",
      "the: the\n",
      "NN\n",
      "function: function\n",
      "NN\n",
      "“: “\n",
      "VB\n",
      "Lemmatize: Lemmatize\n",
      "NN\n",
      "”: ”\n",
      "IN\n",
      "in: in\n",
      "DT\n",
      "this: this\n",
      "NN\n",
      "file: file\n",
      "IN\n",
      "from: from\n",
      "NN\n",
      "question: question\n",
      "NN\n",
      "Create: Create\n",
      "DT\n",
      "a: a\n",
      "NN\n",
      "function: function\n",
      "VBN\n",
      "named: named\n",
      "NN\n",
      "“: “\n",
      "NN\n",
      "Refine: Refine\n",
      "NN\n",
      "”: ”\n",
      "WDT\n",
      "which: which\n",
      "NNS\n",
      "accepts: accepts\n",
      "DT\n",
      "a: a\n",
      "NN\n",
      "string: string\n",
      "CC\n",
      "and: and\n",
      "NN\n",
      "call: call\n",
      "DT\n",
      "the: the\n",
      "IN\n",
      "above: above\n",
      "NNS\n",
      "functions: function\n",
      "IN\n",
      "in: in\n",
      "DT\n",
      "the: the\n",
      "JJ\n",
      "same: same\n",
      "NN\n",
      "order: order\n",
      "NN\n",
      "ie: ie\n",
      "RB\n",
      "first: first\n",
      "VB\n",
      "Tokenize: Tokenize\n",
      "RB\n",
      "then: then\n",
      "NNS\n",
      "RemoveStopWords: RemoveStopWords\n",
      "RB\n",
      "then: then\n",
      "NNP\n",
      "LemmatizeRemember: LemmatizeRemember\n",
      "NN\n",
      ">: >\n",
      "VBN\n",
      "Inputted: Inputted\n",
      "NN\n",
      "string: string\n",
      "MD\n",
      "will: will\n",
      "VB\n",
      "be: be\n",
      "NN\n",
      "input: input\n",
      "TO\n",
      "to: to\n",
      "VB\n",
      "Tokenize: Tokenize\n",
      "NN\n",
      "Function: Function\n",
      "NN\n",
      ">: >\n",
      "VBN\n",
      "Tokenized: Tokenized\n",
      "VBG\n",
      "String: String\n",
      "MD\n",
      "will: will\n",
      "VB\n",
      "be: be\n",
      "NN\n",
      "input: input\n",
      "TO\n",
      "to: to\n",
      "NNS\n",
      "RemoveStopWords: RemoveStopWords\n",
      "NN\n",
      "function: function\n",
      "NN\n",
      ">: >\n",
      "VBN\n",
      "StopWordsRemoved: StopWordsRemoved\n",
      "NN\n",
      "string: string\n",
      "MD\n",
      "will: will\n",
      "VB\n",
      "be: be\n",
      "NN\n",
      "input: input\n",
      "TO\n",
      "to: to\n",
      "VB\n",
      "Lemmatize: Lemmatize\n",
      "NN\n",
      "functionSave: functionSave\n",
      "DT\n",
      "this: this\n",
      "NN\n",
      "python: python\n",
      "NN\n",
      "file: file\n",
      "IN\n",
      "as: a\n",
      "NN\n",
      "PreProcess: PreProcess\n",
      "CC\n",
      "and: and\n",
      "PRP\n",
      "you: you\n",
      "MD\n",
      "can: can\n",
      "NN\n",
      "use: use\n",
      "DT\n",
      "this: this\n",
      "IN\n",
      "for: for\n",
      "VBG\n",
      "upcoming: upcoming\n",
      "NNS\n",
      "assignments: assignment\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Refine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
